{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotutils as pu\n",
    "reload(pu)\n",
    "\n",
    "%matplotlib widget"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "def get_utilization(jobs):\n",
    "    records = [\n",
    "        (float(row['Started']), float(row['Finished']))\n",
    "        for _, row in jobs.iterrows()\n",
    "        if row['Started'] == row['Started'] and row['Finished'] == row['Finished']\n",
    "    ]\n",
    "    records.sort()\n",
    "    sorted_records = []\n",
    "    def get_position(time):\n",
    "        if time < sorted_records[0][0]:\n",
    "            return (-1, False)\n",
    "        else:\n",
    "            length = len(sorted_records)\n",
    "            for i in range(length):\n",
    "                if time >= sorted_records[i][0] and time <= sorted_records[i][1]:\n",
    "                    return (i, True)\n",
    "                elif i < length-1 and time > sorted_records[i][1] and time < sorted_records[i+1][0]:\n",
    "                    return (i, False)\n",
    "            if time > sorted_records[length-1][1]:\n",
    "                return (length-1, False)\n",
    "            else:\n",
    "                return (-2, False)\n",
    "    for record in records:\n",
    "        if len(sorted_records)==0:\n",
    "            sorted_records.append(record)\n",
    "        else:\n",
    "            start_time = record[0]\n",
    "            end_time = record[1]\n",
    "            start_idx, start_wh = get_position(start_time)\n",
    "            end_idx, end_wh = get_position(end_time)\n",
    "            if start_idx == end_idx and not start_wh and not end_wh:\n",
    "                sorted_records.insert(start_idx+1, record)\n",
    "            elif start_wh:\n",
    "                new_start = sorted_records[start_idx][0]\n",
    "                new_end = sorted_records[end_idx][1] if end_wh else end_time\n",
    "                del sorted_records[start_idx:end_idx+1]\n",
    "                sorted_records.insert(start_idx, (new_start, new_end))\n",
    "            else:\n",
    "                new_start = start_time\n",
    "                new_end = sorted_records[end_idx][1] if end_wh else end_time\n",
    "                del sorted_records[start_idx+1:end_idx+1]\n",
    "                sorted_records.insert(start_idx+1, (new_start, new_end))\n",
    "    total_time = sorted_records[-1][1] - sorted_records[0][0]\n",
    "    run_time_list = list(map(lambda x: x[1]-x[0], sorted_records))\n",
    "    run_time = sum(run_time_list)\n",
    "    run_per = run_time / total_time\n",
    "    idle_per = 1 - run_per\n",
    "    return run_per, idle_per\n",
    "\n",
    "\n",
    "def show(jobs, figsize=(10,8)):\n",
    "    fig = plt.figure(figsize=figsize, constrained_layout=True)\n",
    "    axs = fig.subplot_mosaic('''AC\n",
    "                                AC\n",
    "                                BC\n",
    "                                DC\n",
    "                                ''')\n",
    "    # execution\n",
    "    ax = pu.job_timeline(jobs.JobId, jobs.Admitted/1000, jobs.Deadline/1000, ax=axs['C'], numeric_workers=True)\n",
    "    done = jobs[jobs.State == 'done']\n",
    "    ax = pu.job_timeline(done.JobId, done.Started/1000, done.Finished/1000, ax=axs['C'], numeric_workers=True)\n",
    "    ax.set_title('Execution Timeline')\n",
    "    ax.set_ylabel('Job')\n",
    "    ax.set_xlabel('Time') \n",
    "    \n",
    "    # latency CDF\n",
    "    # for the sake of this CDF, set the latency of requets past due to Nan\n",
    "    tmp = jobs.copy()\n",
    "    tmp.loc[tmp.State != 'done', 'Latency'] = np.nan\n",
    "    ax = pu.cdf(tmp.Latency, ax=axs['A'])\n",
    "    ax.set_ylabel('CDF')\n",
    "    ax.set_xlabel('Latency')\n",
    "    \n",
    "    # done\n",
    "    ax = axs['B']\n",
    "    total = len(jobs)\n",
    "    states = ['past_due', 'done']\n",
    "    pos = np.arange(len(states))\n",
    "    data = [\n",
    "        len(jobs.State[jobs.State == state]) / total\n",
    "        for state in states\n",
    "        ]\n",
    "    ax.barh(pos, data, align='center')\n",
    "    for x, y in zip(data, pos):\n",
    "        pu.bar_show_data(x, y, data_y=x, fmt='{:.0%}', ax=ax, xytext=(4, 0), horizontalalignment='left', verticalalignment='center')\n",
    "    pu.cleanup_axis_categorical(ax.yaxis, states)\n",
    "    pu.cleanup_axis_percent(ax.xaxis, xmax=1.0)\n",
    "    ax.set_xlabel('Percentage')\n",
    "\n",
    "    # utilization\n",
    "    ax = axs['D']\n",
    "    states = ['idle', 'run']\n",
    "    pos = np.arange(len(states))\n",
    "    data = get_utilization(jobs)\n",
    "    ax.barh(pos, data, align='center')\n",
    "    for x, y in zip(data, pos):\n",
    "        pu.bar_show_data(x, y, data_y=x, fmt='{:.0%}', ax=ax, xytext=(4, 0), horizontalalignment='left', verticalalignment='center')\n",
    "    pu.cleanup_axis_categorical(ax.yaxis, states)\n",
    "    pu.cleanup_axis_percent(ax.xaxis, xmax=1.0)\n",
    "    ax.set_xlabel('Percentage')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, axs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "incoming = pd.read_csv(Path('../log/bs5') / 'fifo' / 'jobs.csv')\n",
    "outcoming = pd.DataFrame(incoming, columns=[\"JobId\", \"Admitted\", \"Deadline\"])\n",
    "num = incoming.shape[0]\n",
    "file = open(\"news.en\", \"r\")\n",
    "lines = np.array(file.readlines())\n",
    "total_size = len(lines)\n",
    "index = np.random.randint(0, total_size, num)\n",
    "inputs = lines[index]\n",
    "outcoming[\"InputSen\"] = pd.Series(inputs, index = outcoming.index)\n",
    "outcoming.to_csv(\"req.csv\", index=False)"
   ],
   "outputs": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python async_dynamic_test.py --reqs req.csv --output output.csv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clipper = incoming.copy().drop(columns=[\"Length\", \"LengthP99\"])\n",
    "clipper.head()\n",
    "clipper_rst = pd.read_csv('output.csv', sep=':')\n",
    "clipper_rst.loc[clipper_rst[\"OutputSen\"]==\"Non\", \"State\"]= \"past_due\"\n",
    "vl = clipper_rst.shape[0]\n",
    "clipper[\"Latency\"] = np.nan\n",
    "clipper[\"Started\"].iloc[0:vl] = clipper_rst[\"Timestamp\"]\n",
    "clipper[\"Latency\"].iloc[0:vl] = clipper_rst[\"LatencyMS\"]\n",
    "clipper[\"Started\"].iloc[vl:] = np.nan\n",
    "clipper[\"Latency\"].iloc[vl:] = np.nan\n",
    "clipper[\"Finished\"] = clipper[\"Started\"] + clipper[\"Latency\"]\n",
    "clipper[\"State\"].iloc[0:vl] = clipper_rst[\"State\"]\n",
    "clipper[\"State\"].iloc[vl:] = \"past_due\"\n",
    "clipper.loc[clipper[\"Finished\"]>clipper[\"Deadline\"], \"State\"]= \"past_due\"\n",
    "clipper.loc[clipper[\"Finished\"]<=clipper[\"Deadline\"], \"State\"]= \"done\"\n",
    "clipper.loc[clipper[\"State\"]==\"error\", \"State\"]= \"past_due\"\n",
    "clipper.loc[clipper[\"Finished\"]>clipper[\"Deadline\"], \"Latency\"]= np.nan\n",
    "clipper.to_csv(\"final_result.csv\", index=False)\n",
    "clipper.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "show(clipper)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('clipper': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "717f4a39b0fdd059262c701c2d028a945d9b49ac3884e10e20f795d0954b41a8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
